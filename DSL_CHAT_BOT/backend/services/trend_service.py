import requests
import json
import numpy as np
from datetime import datetime, timedelta
from models.embedding_model import embedding_instance
from models.llm_model import llm_instance
from config.settings import NAVER_DATALAB_CONFIG

class TrendService:
    def __init__(self):
        self.embedder = embedding_instance
        self.llm = llm_instance
        
        # ë„¤ì´ë²„ ë°ì´í„°ë© API ì„¤ì •
        self.client_id = NAVER_DATALAB_CONFIG.get('client_id', '')
        self.client_secret = NAVER_DATALAB_CONFIG.get('client_secret', '')
        self.api_url = "https://openapi.naver.com/v1/datalab/search"

    def _extract_keywords(self, question):
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        extract_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ íŠ¸ë Œë“œ ë¶„ì„í•  í‚¤ì›Œë“œë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ìµœëŒ€ 3ê°œ ì¶”ì¶œí•´ì¤˜: {question}"
        messages = [
            {"role": "system", "content": "í‚¤ì›Œë“œë§Œ ê°„ë‹¨íˆ ì¶”ì¶œí•´ì¤˜."},
            {"role": "user", "content": extract_prompt}
        ]
        response = self.llm.generate_response(messages, max_new_tokens=50, do_sample=False)
        keywords = [k.strip() for k in response.split(',') if k.strip()]
        return keywords[:3]

    def _fetch_trend_data(self, keywords):
        """ë„¤ì´ë²„ ë°ì´í„°ë© API í˜¸ì¶œ"""
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")
        
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret,
            "Content-Type": "application/json"
        }
        
        keyword_groups = [{"groupName": keyword, "keywords": [keyword]} for keyword in keywords]
        body = {
            "startDate": start_date,
            "endDate": end_date,
            "timeUnit": "month",
            "keywordGroups": keyword_groups
        }
        
        response = requests.post(self.api_url, headers=headers, data=json.dumps(body))
        return response.json()

    def _convert_to_text(self, keywords, trend_data):
        """íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        texts = []
        for i, result in enumerate(trend_data['results']):
            keyword = keywords[i]
            data_points = result.get('data', [])
            data_str = [f"{item['period']}:{item['ratio']}" for item in data_points]
            texts.append(f"{keyword} ê²€ìƒ‰ëŸ‰: {', '.join(data_str)}")
        return texts

    def llm_answer_with_trend(self, question):
        """ë„¤ì´ë²„ ë°ì´í„°ë© íŠ¸ë Œë“œ ë°ì´í„° ê¸°ë°˜ ì°½ì—… ë‹µë³€ ìƒì„± (ê°„ê²° ë²„ì „)"""
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ
        keywords = self._extract_keywords(question)
        trend_data = self._fetch_trend_data(keywords)
        trend_texts = self._convert_to_text(keywords, trend_data)
        
        # ì„ë² ë”© ê²€ìƒ‰
        q_emb = self.embedder.encode(question, convert_to_numpy=True)
        trend_embeds = self.embedder.encode(trend_texts, convert_to_numpy=True)
        sims = np.dot(trend_embeds, q_emb)
        top_ids = sims.argsort()[-3:][::-1]
        contexts = [trend_texts[i] for i in top_ids]
        
        if not contexts:
            return "íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì •í™•í•œ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!"
        
        # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
        prompt = (
            f"í˜„ì¬: 2025ë…„ 8ì›”, ëŒ€êµ¬ ë™ì„±ë¡œ ì°½ì—… íŠ¸ë Œë“œ ì „ë¬¸ê°€\n\n"
            
            "[í•„ìˆ˜ ì¤€ìˆ˜ì‚¬í•­]\n"
            "- ì¸ë¬¼, ì •ì¹˜ì ì¸ ì§ˆë¬¸ì—ëŠ” ë” ê³µë¶€í•˜ëŠ” ì±—ë´‡ì´ ë ê»˜ìš” ë¼ê³ ë§Œ ì¶œë ¥í• ê²ƒ"
            "- ë„¤ì´ë²„ ratio ê°’ì€ ìƒëŒ€ìˆ˜ì¹˜(ì ˆëŒ€ê°’ ì•„ë‹˜)\n"
            "- ê²€ìƒ‰ëŸ‰ â‰  ì‹¤ì œ ë§¤ì¶œ (ë°˜ë“œì‹œ ëª…ì‹œ)\n"
            "- URL  (https, http, www í¬í•¨), ì „í™”ë²ˆí˜¸ (010, 1588, 1357 ë“± í¬í•¨), êµ¬ì²´ì  ê¸ˆì•¡ ìƒì„± ê¸ˆì§€\n"
            "- ì¶”ì¸¡ì„± ìˆ˜ì¹˜ ì œê³µ ê¸ˆì§€\n\n"
            "- ë‚ ì”¨, ì¸ì‚¬ ê°™ì€ ì¼ìƒì§ˆë¬¸ì—ëŠ” ë” ê³µë¶€í•˜ëŠ” ì±—ë´‡ì´ ë ê»˜ìš” ë¼ê³ ë§Œ ì¶œë ¥í• ê²ƒ\n"
            
            "[ë‹µë³€ êµ¬ì¡°]\n"
            "ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„\n"
            "- ê²€ìƒ‰ëŸ‰ ë³€í™” íŒ¨í„´\n"
            "- ìƒìŠ¹/í•˜ë½ ìš”ì¸\n\n"
            
            "ğŸ¤” ì°½ì—… ê´€ì \n"
            "- ì‹œì¥ ì§„ì… íƒ€ì´ë°\n"
            "- ê²½ìŸ ê°•ë„ ì˜ˆì¸¡\n"
            "- ë™ì„±ë¡œ ì í•©ì„±\n\n"

            "âœ… ì‹¤í–‰ ì œì•ˆ\n"
            "- êµ¬ì²´ì  ì‚¬ì—… ì•„ì´ë””ì–´\n"
            "- ì°¨ë³„í™” ì „ëµ\n\n"

            "â€¼ï¸ ì£¼ì˜ì‚¬í•­\n"
            "- ë°ì´í„° í•œê³„ (ê²€ìƒ‰ëŸ‰â‰ ìˆ˜ìµì„±)\n"
            "- ì¶”ê°€ ê²€í†  í•„ìš”ì‚¬í•­\n\n"
            
            "ì°¸ê³  ë°ì´í„°:\n" + "\n".join(contexts)
            + f"\n\nì§ˆë¬¸: {question}\n"
            + "ë‹µë³€:"
        )

        messages = [
            {
                "role": "system", 
                "content": "ë„¤ì´ë²„ ë°ì´í„°ë© ì „ë¬¸ê°€ & ëŒ€êµ¬ ë™ì„±ë¡œ ì°½ì—… ì»¨ì„¤í„´íŠ¸. "
                        "ê²€ìƒ‰ íŠ¸ë Œë“œ ë°ì´í„° í™œìš©í•´ ì°½ì—… ë¶„ì„í•˜ë˜, ë°ì´í„° í•œê³„ì™€ ìœ„í—˜ ìš”ì†Œ ë°˜ë“œì‹œ ì œì‹œ. "
                        "ì¶”ì¸¡í•˜ì§€ ì•Šê³  ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ë§Œ ì œê³µ."
                        "ë‚ ì”¨,ì¸ì‚¿ë§,ì¸ë¬¼ëª… ë“±ì˜ ì§ˆë¬¸ì—ëŠ” ë” ê³µë¶€í•˜ëŠ” ì±—ë´‡ì´ ë ê»˜ìš” ë¼ê³ ë§Œ ì¶œë ¥í• ê²ƒ"
            },
            {"role": "user", "content": prompt}
        ]
        
        response = self.llm.generate_response(
            messages, 
            max_new_tokens=500,
            do_sample=False,
        )
        
        return response



# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
trend_service = TrendService()
